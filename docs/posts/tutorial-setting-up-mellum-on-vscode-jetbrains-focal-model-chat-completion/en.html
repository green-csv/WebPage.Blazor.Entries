<h1 id="contents">Contents</h1>
<ul>
<li><a href="entry/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion#tutorial-setting-up-and-running-mellum">Tutorial: Setting Up and Running Mellum</a></li>
<li><a href="entry/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion#add-mellum-to-ollama">Add Mellum to Ollama</a></li>
<li><a href="entry/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion#vs-code-continue--ollama">VS Code, Continue &#x26; Ollama</a></li>
</ul>
<h1 id="tutorial-setting-up-and-running-mellum">Tutorial: Setting Up and Running Mellum</h1>
<ol>
<li>Clone the Mellum model files</li>
</ol>
<pre><code class="hljs language-bash">   git <span class="hljs-built_in">clone</span> https://huggingface.co/JetBrains/Mellum-4b-base-gguf
</code></pre>
<p>(~4.5 GB)<sup><a href="entry/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></p>
<ol start="2">
<li>
<p>Install Ollama<sup><a href="entry/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion#user-content-fn-7" id="user-content-fnref-7" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>
Download and install the appropriate Ollama package for your OS (Ollama, 2025).</p>
</li>
<li>
<p>Install Continue<sup><a href="entry/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion#user-content-fn-10" id="user-content-fnref-10" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> in VS Code
Add the Continue extension to wire up Mellum via Ollama.</p>
</li>
</ol>
<h1 id="add-mellum-to-ollama">Add Mellum to Ollama</h1>
<ol>
<li>In your project directory, create a file named <code>Modelfile</code> with:</li>
</ol>
<pre><code class="hljs language-text">FROM ./mellum-4b-base.Q8_0.gguf
# PARAMETER &#x3C;parameter> &#x3C;value>

SYSTEM "You are a completion assistant. Predict the most likely continuation of the given input accurately and concisely."

PARAMETER temperature      0.2
PARAMETER top_p            0.9
PARAMETER top_k            40
PARAMETER repeat_penalty   1.2
PARAMETER repeat_last_n    128
PARAMETER num_predict      512
PARAMETER num_ctx          8192

PARAMETER stop "###"
PARAMETER stop "\n\n"

</code></pre>
<p>All of these directives and parameters come from the Ollama Modelfile<sup><a href="entry/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup></p>
<ol start="2">
<li>(Optional) To run a quantized build on modest hardware at the expense of some accuracy, see Ollama’s quantization guide:</li>
</ol>
<blockquote>
<p><a href="https://ollama.readthedocs.io/en/import/#quantizing-a-model">https://ollama.readthedocs.io/en/import/#quantizing-a-model</a></p>
</blockquote>
<ol start="3">
<li>Create your Ollama model</li>
</ol>
<pre><code class="hljs language-pws">ollama create Mellum -f .\Modelfile`
</code></pre>
<p>Make sure the <code>FROM</code> base matches the one used to the Model file (Chang, n.d.).</p>
<ol start="4">
<li>Inspect default values</li>
</ol>
<pre><code>ollama show Mellum
</code></pre>
<p>You’ll see something like:</p>
<pre><code> Model
    architecture        llama
    parameters          4.0B
    context length      8192
    embedding length    3072
    quantization        Q8_0

  Capabilities
    completion

  Parameters
    top_k             40
    top_p             0.9
    num_ctx           8192
    num_predict       512
    repeat_last_n     128
    repeat_penalty    1.2
    stop              "###"
    stop              "\n\n"
    temperature       0.2

</code></pre>
<p>Tweak any settings in your <code>Modelfile</code>, then rebuild with <code>ollama create</code> as above.</p>
<h1 id="vs-code-continue--ollama">VS Code, Continue &#x26; Ollama</h1>
<p><strong>Continue</strong><sup><a href="entry/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion#user-content-fn-10" id="user-content-fnref-10-2" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> lets you hook any local or remote model into VS Code, select Ollama as your provider, then Add a Local Model even though it isn’t in the Ollama registry by default. Configure per-context assistants from the Continue UI, and you’ll get Mellum focal context completions right in VS Code.</p>
<p><img src="https://github.com/green-csv/WebPage.Blazor.Entries/blob/main/posts/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion/img-20250522093033.png" alt="Continue configuration for Ollama + Mellum "></p>
<p>Overall, on my station with a dedicated GPU it’s fast, but on smaller machines you can quantize the model to fit your needs. Now I can zip between Unity and VS Code with context-aware completions via Mellum.</p>
<p>In continue configured for each context available a here are some insights</p>
<p><img src="https://github.com/green-csv/WebPage.Blazor.Entries/blob/main/posts/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion/GameManager.mp4" alt="Mellum Context completion"></p>
<p><img src="https://github.com/green-csv/WebPage.Blazor.Entries/blob/main/posts/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion/ShiInputManager.mp4" alt="Mellum Instructions"></p>
<section data-footnotes="" class="footnotes"><h2 class="sr-only" id="footnote-label">Footnotes</h2>
<ol>
<li id="user-content-fn-3">
<p>JetBrains. (2025b, April). Mellum goes open source: A purpose‐built LLM for developers, now on Hugging Face. <em>JetBrains Blog.</em> Retrieved May 21, 2025, from <a href="https://blog.jetbrains.com/ai/2025/04/mellum-goes-open-source-a-purpose-built-llm-for-developers-now-on-hugging-face/">https://blog.jetbrains.com/ai/2025/04/mellum-goes-open-source-a-purpose-built-llm-for-developers-now-on-hugging-face/</a> <a href="entry/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-7">
<p>Ollama. (2025). Download Ollama on Windows [Software]. Retrieved May 21, 2025, from <a href="https://ollama.com/download/windows">https://ollama.com/download/windows</a> <a href="entry/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion#user-content-fnref-7" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-10">
<p>Continue.dev. (2025). <em>Continue – Open-source AI code assistant</em> [Extension]. Visual Studio Marketplace. Retrieved May 21, 2025, from <a href="https://marketplace.visualstudio.com/items?itemName=Continue.continue">https://marketplace.visualstudio.com/items?itemName=Continue.continue</a> <a href="entry/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion#user-content-fnref-10" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">↩</a> <a href="entry/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion#user-content-fnref-10-2" data-footnote-backref="" aria-label="Back to reference 3-2" class="data-footnote-backref">↩<sup>2</sup></a></p>
</li>
<li id="user-content-fn-1">
<p>Chang, L. (n.d.). <em>Model file specification Ollama</em> [Documentation]. GitHub. Retrieved May 21, 2025, from <a href="https://github.com/lloydchang/ollama-ollama/blob/main/docs/modelfile.md">https://github.com/lloydchang/ollama-ollama/blob/main/docs/modelfile.md</a> <a href="entry/tutorial-setting-up-mellum-on-vscode-jetbrains-focal-model-chat-completion#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section>